# FSE 2017 Artifacts Track

## Scope

This site indexes the accepted artifacts of FSE 2017 research track papers. According to [ACM's "Result and Artifact Review and Badging" policy](​https://www.acm.org/publications/policies/artifact-review-badging), an *artifact* is ``a digital object that was either created by the authors to be used as part of the study or generated by the experiment itself [...] software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results``. A formal review of such artifacts not only ensures that the study is repeatable by the same team, if they are available online then other researchers can replicate the findings as well.

In this spirit, the ESEC/FSE 2017 artifacts track exists to review, promote, share and catalog the research artifacts produced by any of the papers accepted to the research track. Apart from repeatability and replicability, cataloguing these artifacts also allows reuse by other teams in reproduction or other studies. Artifacts of interest include (but are not limited to):

* Tools, which are implementations of systems or algorithms potentially useful in other studies.
* Data repositories, which are data (e.g., logging data, system traces, survey raw data) that can be used for multiple software engineering approaches.
* Frameworks, which are tools and services illustrating new approaches to software engineering that could be used by other researchers in different contexts.

## Review Process

Out of 62 research track papers (each eligible to submit to the artifacts track), we received 13 artifact submissions, ranging from data sets to benchmarks or tools. Each artifact was reviewed by 3 artifact PC members, after which authors were able to react to these reviews by fixing their artifacts (or their documentation). Subsequent re-reviewing and discussion led to 11 artifacts being accepted. Review criteria, more details as well as the list of artifact PC members can be found on the [conference web site](http://esec-fse17.uni-paderborn.de/call_artifacts.php).

Out of these 11 accepted artifacts, 5 received an "Artifacts Evaluated - Reusable" badge, and 6 an "Artifacts Evaluated - Functional" badge. These badges are defined by the [ACM](​https://www.acm.org/publications/policies/artifact-review-badging) as:

* Artifacts Evaluated - Functional: The artifacts are complete, well-documented and allow to obtain the same results as the paper.
* Artifacts Evaluated - Reusable: As above, but the artifacts are of such a high quality that they can be reused as is on other data sets, or for other purposes.

These badges are tagged onto the corresponding research track papers and visible in the ACM Digital Library. Furthermore, authors were asked to ensure that the artifacts are available from a stable URL or DOI (i.e., not a personal website) for anyone to access. Some kind of 5 year-archival plan (at least) should be provided.

Below, we list the 11 accepted artifacts, grouped by badge and ordered by title.

## Artifacts Evaluated - Reusable

* Tegan Brennan, Nestan Tsiskaridze, Nicolas Rosner, Abdulbaki Aydin and Tevfik Bultan. [Constraint Normalization and Parameterized Caching for Quantitative Program Analysis](https://cashew.vlab.cs.ucsb.edu/)
* Alexander Kn&uuml;ppel, Thomas Th&uuml;m, Stephan Mennicke, Jens Meinicke and Ina Schaefer. [Software Artifacts: Is There a Mismatch Between Real-World Feature Models and Product-Line Research?](https://github.com/AlexanderKnueppel/is-there-a-mismatch)
* David Bingham Brown, Ben Liblit, Thomas Reps and Michael Vaughn. [The Care and Feeding of Wild-Caught Mutants](https://github.com/d-bingham/fse2017artifact)
* Tim Nelson, Natasha Danas, Daniel Dougherty and Shriram Krishnamurthi. [The Power of "Why" and "Why not": Enriching Scenario Exploration with Provenance](http://cs.brown.edu/research/plt/dl/fse2017/)
* Marcel B&ouml;hme, Ezekiel Olamide Soremekun, Sudipta Chattopadhyay, Emamurho Ugherughe and Andreas Zeller. [Where is the Bug and How is it Fixed? An Experiment with Practitioners](https://dbgbench.github.io/)

## Artifacts Evaluated - Functional

* Adarsh Yoga and Santosh Nagarakatte. [A Fast Causal Profiler for Task Parallel Programs](https://github.com/rutgers-apl/TaskProf)
* Jooyong Yi, Umair Ahmed, Amey Karkare, Shin Hwei Tan and Abhik Roychoudhury. [A Feasibility Study of Using Automated Program Repair for Introductory Programming Assignments](https://github.com/jyi/ITSP)
* Fan Long, Peter Amidon and Martin Rinard. [Automatic Inference of Code Transforms for Patch Generation Systems](http://rhino.csail.mit.edu/genesis-rep/)
* Fabrizio Pastore, Leonardo Mariani and Daniela Micucci. [BCDI: Behavioral Driven Conflict Identification](https://github.com/pastoref/bdci)
* Mojtaba Bagherzadeh, Nicolas Hili and Juergen Dingel. [Model-level, Platform-independent Debugging in the Context of the Model-driven Development of Real-time Systems](https://github.com/moji1/MDebugger)
* Alireza Sadeghi, Reyhaneh Jabbarvand and Sam Malek. [PATDroid: Permission-Aware GUI Testing of Android](https://sites.google.com/view/patdroid)


